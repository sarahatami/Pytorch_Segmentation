{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMst8LhyFLqkHPyfnvtFaHe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarahatami/Pytorch_Segmentation/blob/master/Supervised_Pytorch_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KULR_-aV4CO9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "from os import listdir\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torchvision.models.segmentation as seg\n",
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import sys\n",
        "from collections import namedtuple\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# city escape pictures\n",
        "! wget --keep-session-cookies --save-cookies=cookies.txt --post-data 'username=sarahatami&password=...&submit=Login' https://www.cityscapes-dataset.com/login/\n",
        "! wget --load-cookies cookies.txt --content-disposition https://www.cityscapes-dataset.com/file-handling/?packageID=1 #gt\n",
        "! wget --load-cookies cookies.txt --content-disposition https://www.cityscapes-dataset.com/file-handling/?packageID=3 #image"
      ],
      "metadata": {
        "id": "wI2KbWfO4y76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/gtFine_trainvaltest.zip\n",
        "!rm /content/README\n",
        "!rm /content/index.html\n",
        "!rm /content/license.txt\n",
        "!unzip /content/leftImg8bit_trainvaltest.zip\n",
        "!rm /content/gtFine_trainvaltest.zip\n",
        "!rm /content/leftImg8bit_trainvaltest.zip"
      ],
      "metadata": {
        "id": "oXLNLFHw41X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Label = namedtuple( 'Label' , ['name', 'id', 'trainId', 'category', 'categoryId', 'hasInstances', 'ignoreInEval', 'color',  ] )\n",
        "labels = [\n",
        "    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n",
        "    Label(  'unlabeled'            ,  0 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'ego vehicle'          ,  1 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'rectification border' ,  2 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'out of roi'           ,  3 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'static'               ,  4 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'dynamic'              ,  5 ,      255 , 'void'            , 0       , False        , True         , (111, 74,  0) ),\n",
        "    Label(  'ground'               ,  6 ,      255 , 'void'            , 0       , False        , True         , ( 81,  0, 81) ),\n",
        "    Label(  'road'                 ,  7 ,        0 , 'flat'            , 1       , False        , False        , (128, 64,128) ),\n",
        "    Label(  'sidewalk'             ,  8 ,        1 , 'flat'            , 1       , False        , False        , (244, 35,232) ),\n",
        "    Label(  'parking'              ,  9 ,      255 , 'flat'            , 1       , False        , True         , (250,170,160) ),\n",
        "    Label(  'rail track'           , 10 ,      255 , 'flat'            , 1       , False        , True         , (230,150,140) ),\n",
        "    Label(  'building'             , 11 ,        2 , 'construction'    , 2       , False        , False        , ( 70, 70, 70) ),\n",
        "    Label(  'wall'                 , 12 ,        3 , 'construction'    , 2       , False        , False        , (102,102,156) ),\n",
        "    Label(  'fence'                , 13 ,        4 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n",
        "    Label(  'guard rail'           , 14 ,      255 , 'construction'    , 2       , False        , True         , (180,165,180) ),\n",
        "    Label(  'bridge'               , 15 ,      255 , 'construction'    , 2       , False        , True         , (150,100,100) ),\n",
        "    Label(  'tunnel'               , 16 ,      255 , 'construction'    , 2       , False        , True         , (150,120, 90) ),\n",
        "    Label(  'pole'                 , 17 ,        5 , 'object'          , 3       , False        , False        , (153,153,153) ),\n",
        "    Label(  'polegroup'            , 18 ,      255 , 'object'          , 3       , False        , True         , (153,153,153) ),\n",
        "    Label(  'traffic light'        , 19 ,        6 , 'object'          , 3       , False        , False        , (250,170, 30) ),\n",
        "    Label(  'traffic sign'         , 20 ,        7 , 'object'          , 3       , False        , False        , (220,220,  0) ),\n",
        "    Label(  'vegetation'           , 21 ,        8 , 'nature'          , 4       , False        , False        , (107,142, 35) ),\n",
        "    Label(  'terrain'              , 22 ,        9 , 'nature'          , 4       , False        , False        , (152,251,152) ),\n",
        "    Label(  'sky'                  , 23 ,       10 , 'sky'             , 5       , False        , False        , ( 70,130,180) ),\n",
        "    Label(  'person'               , 24 ,       11 , 'human'           , 6       , True         , False        , (220, 20, 60) ),\n",
        "    Label(  'rider'                , 25 ,       12 , 'human'           , 6       , True         , False        , (255,  0,  0) ),\n",
        "    Label(  'car'                  , 26 ,       13 , 'vehicle'         , 7       , True         , False        , (  0,  0,142) ),\n",
        "    Label(  'truck'                , 27 ,       14 , 'vehicle'         , 7       , True         , False        , (  0,  0, 70) ),\n",
        "    Label(  'bus'                  , 28 ,       15 , 'vehicle'         , 7       , True         , False        , (  0, 60,100) ),\n",
        "    Label(  'caravan'              , 29 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0, 90) ),\n",
        "    Label(  'trailer'              , 30 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0,110) ),\n",
        "    Label(  'train'                , 31 ,       16 , 'vehicle'         , 7       , True         , False        , (  0, 80,100) ),\n",
        "    Label(  'motorcycle'           , 32 ,       17 , 'vehicle'         , 7       , True         , False        , (  0,  0,230) ),\n",
        "    Label(  'bicycle'              , 33 ,       18 , 'vehicle'         , 7       , True         , False        , (119, 11, 32) ),\n",
        "  # Label(  'license plate'        , -1 ,       -1 , 'vehicle'         , 7       , False        , True         , (  0,  0,142) ),\n",
        "    Label(  'car'                  , 26 ,       -1 , 'vehicle'         , 7       , False        , True         , (  0,  0,142) ),\n",
        "]"
      ],
      "metadata": {
        "id": "4yI1MJEy45h1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(threshold = np.inf)\n",
        "transform_ann = transforms.Compose([transforms.Resize((205, 410))])\n",
        "\n",
        "def gt_to_label(pil_image):\n",
        "  transformed_ann = transform_ann(pil_image)\n",
        "  arr_image = np.array(transformed_ann)\n",
        "  label_map = np.zeros(arr_image.shape[0:2], dtype=np.uint8) # (205, 410)\n",
        "  for label in labels:\n",
        "    i = np.all(arr_image == label.color, axis=-1)\n",
        "    label_map[i] = label.id # (205, 410)\n",
        "  return label_map"
      ],
      "metadata": {
        "id": "XhS7GLKi46cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Defining the Dataset class ###\n",
        "class Cityscapes(Dataset):\n",
        "    def __init__(self, transform=None):\n",
        "        self.transform = transform\n",
        "        self.images_dir = '/content/leftImg8bit/train/bremen'\n",
        "        self.images = sorted(os.listdir(self.images_dir))\n",
        "        self.gt_dir = '/content/gtFine/train/bremen'\n",
        "        self.gts = sorted([f for f in os.listdir(self.gt_dir) if f.endswith(\"color.png\")])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.images_dir, self.images[idx])\n",
        "        gt_path = os.path.join(self.gt_dir, self.gts[idx])\n",
        "\n",
        "        pil_image = Image.open(img_path).convert(\"RGB\")  #<class 'PIL.Image.Image'>\n",
        "        pil_gt = Image.open(gt_path).convert(\"RGB\") #<class 'PIL.Image.Image'>\n",
        "        gt = gt_to_label(pil_gt)\n",
        "        if self.transform:\n",
        "            img = self.transform(pil_image)\n",
        "        return img, gt\n",
        "\n",
        "transform_img = transforms.Compose([transforms.Resize((205, 410)),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "trainset = Cityscapes(transform=transform_img)\n",
        "dataloader_train = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=False)"
      ],
      "metadata": {
        "id": "2Ul7Bjyg4-KB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-6\n",
        "criterion = CrossEntropyLoss()\n",
        "num_epochs = 100"
      ],
      "metadata": {
        "id": "PdG4WP_Z5FJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "color_map = [(  0,  0,  0) ,\n",
        "             (  0,  0,  0) ,\n",
        "             (  0,  0,  0) ,\n",
        "             (  0,  0,  0) ,\n",
        "             (  0,  0,  0) ,\n",
        "             (111, 74,  0) ,\n",
        "             ( 81,  0, 81) ,\n",
        "             (128, 64,128) ,\n",
        "             (244, 35,232) ,\n",
        "             (250,170,160) ,\n",
        "             (230,150,140) ,\n",
        "             ( 70, 70, 70) ,\n",
        "             (102,102,156) ,\n",
        "             (190,153,153) ,\n",
        "             (180,165,180) ,\n",
        "             (150,100,100) ,\n",
        "             (150,120, 90) ,\n",
        "             (153,153,153) ,\n",
        "             (153,153,153) ,\n",
        "             (250,170, 30) ,\n",
        "             (220,220,  0) ,\n",
        "             (107,142, 35) ,\n",
        "             (152,251,152) ,\n",
        "             ( 70,130,180) ,\n",
        "             (220, 20, 60) ,\n",
        "             (255,  0,  0) ,\n",
        "             (  0,  0,142) ,\n",
        "             (  0,  0, 70) ,\n",
        "             (  0, 60,100) ,\n",
        "             (  0,  0, 90) ,\n",
        "             (  0,  0,110) ,\n",
        "             (  0, 80,100) ,\n",
        "             (  0,  0,230) ,\n",
        "             (119, 11, 32) ,\n",
        "             (  0,  0,142) ]"
      ],
      "metadata": {
        "id": "Myi0kBZlWTJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#################################################################"
      ],
      "metadata": {
        "id": "MvSXuoSwi3W6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "****DeepLabV3 model with a ResNet-50 backbone****"
      ],
      "metadata": {
        "id": "qdsOpy5wed4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes=34\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = seg.deeplabv3_resnet50(pretrained=True)\n",
        "# model = seg.lraspp_mobilenet_v3_large( pretrained=True)\n",
        "# model = seg.deeplabv3_mobilenet_v3_large(pretrained=True)\n",
        "\n",
        "last_layer = model.classifier[-1]\n",
        "model.classifier[4]=torch.nn.Conv2d(256, num_classes, kernel_size=(1, 1), stride=(1, 1))\n",
        "model = model.to(device)\n",
        "optimizer = Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "dTcYPAGKef3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for i, data in enumerate(dataloader_train):\n",
        "        inputs, annotations = data # [bs, 3, 205, 410], [bs, 205, 410]\n",
        "        inputs = inputs.to(device)\n",
        "        annotations = annotations.to(device).long()\n",
        "        # inputs=torch.autograd.Variable(inputs,requires_grad=False)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        model.train()\n",
        "        outputs = model(inputs)['out']\n",
        "\n",
        "        loss = criterion(outputs, annotations)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"model is at epoch {epoch}, step {i}\")\n",
        "torch.save(model.state_dict(), f\"{epoch}.torch\")"
      ],
      "metadata": {
        "id": "j0W7cATVfHJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "model = seg.deeplabv3_resnet50(pretrained=True)\n",
        "model.classifier[4]=torch.nn.Conv2d(256, num_classes, kernel_size=(1, 1), stride=(1, 1))\n",
        "\n",
        "model_path=\"/content/99.torch\"\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "for i, data in enumerate(dataloader_train):\n",
        "  images, annotations = data\n",
        "  images = images.to(device)\n",
        "  outputs = model(images)['out'] #[4, 34, 205, 410]\n",
        "\n",
        "  segm = torch.argmax(outputs, 1).cpu().detach().numpy() #(4, 205, 410)\n",
        "  labelmap=segm[0,:,:]\n",
        "  rgbimage = np.zeros((205, 410, 3), dtype=np.uint8)\n",
        "\n",
        "  for i in range(labelmap.shape[0]):\n",
        "    for j in range(labelmap.shape[1]):\n",
        "        color_index = labelmap[i, j]\n",
        "        rgbimage[i, j] = color_map[color_index]  # Assign the corresponding RGB color\n",
        "\n",
        "  out_image = Image.fromarray(rgbimage)\n",
        "  plt.imshow(out_image)\n",
        "  plt.show()\n",
        "  # stop"
      ],
      "metadata": {
        "id": "4Phw-4XMfivX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "######################################################################"
      ],
      "metadata": {
        "id": "xOt4LS5ah1vI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "****Lite R-ASPP Network model with a MobileNetV3-Large backbone****"
      ],
      "metadata": {
        "id": "YdGIUMdL62fJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes=34\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# model = seg.deeplabv3_resnet50(pretrained=True)\n",
        "model = seg.lraspp_mobilenet_v3_large( pretrained=True)\n",
        "# model = seg.deeplabv3_mobilenet_v3_large(pretrained=True)\n",
        "\n",
        "model.classifier.high_classifier = torch.nn.Conv2d(model.classifier.high_classifier.in_channels\n",
        "                                                   , num_classes, kernel_size=(1, 1), stride=(1, 1))\n",
        "model.classifier.low_classifier = torch.nn.Conv2d(model.classifier.low_classifier.in_channels\n",
        "                                                   , num_classes, kernel_size=(1, 1), stride=(1, 1))\n",
        "model = model.to(device)\n",
        "optimizer = Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "hjqZ0r5e5B3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for i, data in enumerate(dataloader_train):\n",
        "        inputs, annotations = data # [bs, 3, 205, 410], [bs, 205, 410]\n",
        "        inputs = inputs.to(device)\n",
        "        annotations = annotations.to(device).long()\n",
        "        # inputs=torch.autograd.Variable(inputs,requires_grad=False)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        model.train()\n",
        "        outputs = model(inputs)['out']\n",
        "\n",
        "        loss = criterion(outputs, annotations)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print(f\"model at epoch {epoch}, step {i}\")\n",
        "torch.save(model.state_dict(), f\"{epoch}.torch\")"
      ],
      "metadata": {
        "id": "J1leFiNb5HkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "model = seg.deeplabv3_resnet50(pretrained=True)\n",
        "model.classifier[4]=torch.nn.Conv2d(256, num_classes, kernel_size=(1, 1), stride=(1, 1))\n",
        "\n",
        "model_path=\"/content/99.torch\"\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "for i, data in enumerate(dataloader_train):\n",
        "  images, annotations = data\n",
        "  images = images.to(device)\n",
        "  outputs = model(images)['out'] #[4, 34, 205, 410]\n",
        "\n",
        "  segm = torch.argmax(outputs, 1).cpu().detach().numpy() #(4, 205, 410)\n",
        "  labelmap=segm[0,:,:]\n",
        "  rgbimage = np.zeros((205, 410, 3), dtype=np.uint8)\n",
        "\n",
        "  for i in range(labelmap.shape[0]):\n",
        "    for j in range(labelmap.shape[1]):\n",
        "        color_index = labelmap[i, j]\n",
        "        rgbimage[i, j] = color_map[color_index]  # Assign the corresponding RGB color\n",
        "\n",
        "  out_image = Image.fromarray(rgbimage)\n",
        "  plt.imshow(out_image)\n",
        "  plt.show()\n",
        "  # stop"
      ],
      "metadata": {
        "id": "MTNUv-9-WaxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##########################################################################"
      ],
      "metadata": {
        "id": "LKkq-tk6h5zq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ****DeepLabV3 model with a MobileNetV3-Large backbone****"
      ],
      "metadata": {
        "id": "rBOpDQO4ga3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes=34\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# model = seg.deeplabv3_resnet50(pretrained=True)\n",
        "# model = seg.lraspp_mobilenet_v3_large( pretrained=True)\n",
        "model = seg.deeplabv3_mobilenet_v3_large(pretrained=True)\n",
        "\n",
        "model.classifier[4]=torch.nn.Conv2d(256, num_classes, kernel_size=(1, 1), stride=(1, 1))\n",
        "last_layer = model.classifier[-1]\n",
        "model = model.to(device)\n",
        "optimizer = Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "6GuXpKfpeTIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    print(epoch)\n",
        "    for i, data in enumerate(dataloader_train):\n",
        "        inputs, annotations = data # [bs, 3, 205, 410], [bs, 205, 410]\n",
        "        inputs = inputs.to(device)\n",
        "        annotations = annotations.to(device).long()\n",
        "        # inputs=torch.autograd.Variable(inputs,requires_grad=False)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        model.train()\n",
        "        outputs = model(inputs)['out']\n",
        "\n",
        "        loss = criterion(outputs, annotations)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if i % 100 == 0:\n",
        "      print(f\"model is at epoch {epoch}, step {i}\")\n",
        "torch.save(model.state_dict(), f\"{epoch}_2nd.torch\")"
      ],
      "metadata": {
        "id": "8EzlGHwfguzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "model = seg.deeplabv3_resnet50(pretrained=True)\n",
        "model.classifier[4]=torch.nn.Conv2d(256, num_classes, kernel_size=(1, 1), stride=(1, 1))\n",
        "\n",
        "model_path=\"/content/99.torch\"\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "for i, data in enumerate(dataloader_train):\n",
        "  images, annotations = data\n",
        "  images = images.to(device)\n",
        "  outputs = model(images)['out'] #[4, 34, 205, 410]\n",
        "\n",
        "  segm = torch.argmax(outputs, 1).cpu().detach().numpy() #(4, 205, 410)\n",
        "  labelmap=segm[0,:,:]\n",
        "  rgbimage = np.zeros((205, 410, 3), dtype=np.uint8)\n",
        "\n",
        "  for i in range(labelmap.shape[0]):\n",
        "    for j in range(labelmap.shape[1]):\n",
        "        color_index = labelmap[i, j]\n",
        "        rgbimage[i, j] = color_map[color_index]  # Assign the corresponding RGB color\n",
        "\n",
        "  out_image = Image.fromarray(rgbimage)\n",
        "  plt.imshow(out_image)\n",
        "  plt.show()\n",
        "  # stop"
      ],
      "metadata": {
        "id": "vYGRjBWUg535"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}